lld linking issue
Index: pytorch-1.12.0/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
===================================================================
--- pytorch-1.12.0.orig/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
+++ pytorch-1.12.0/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
@@ -234,6 +234,12 @@ void nan_to_num_kernel_cuda(
 }
 
 void frexp_kernel_cuda(TensorIteratorBase& iter) {
+#if defined(USE_ROCM)
+  // Reference: https://rocmdocs.amd.com/en/latest/ROCm_API_References/HIP-MATH.html
+  //            https://github.com/ROCm-Developer-Tools/HIP/issues/2169
+  // ROCm does not support frexp function yet
+  TORCH_CHECK(false, "torch.frexp() is not implemented on ROCm platform.");
+#else
   AT_DISPATCH_FLOATING_TYPES_AND(ScalarType::Half,
     // The iter.dtype() here is the dtype of mantissa output.
     // It's a floating point type and must be the same as the input's dtype.
@@ -245,6 +251,7 @@ void frexp_kernel_cuda(TensorIteratorBas
         return {mantissa, exponent};
       });
   });
+#endif
 }
 
 REGISTER_DISPATCH(bitwise_not_stub, &bitwise_not_kernel_cuda);
Index: pytorch-1.12.0/torch/utils/hipify/cuda_to_hip_mappings.py
===================================================================
--- pytorch-1.12.0.orig/torch/utils/hipify/cuda_to_hip_mappings.py
+++ pytorch-1.12.0/torch/utils/hipify/cuda_to_hip_mappings.py
@@ -37,7 +37,6 @@ MATH_TRANSPILATIONS = collections.Ordere
         ("std::fabs", ("::fabs")),
         ("std::fmod", ("::fmod")),
         ("std::remainder", ("::remainder")),
-        ("std::frexp", ("::frexp")),
     ]
 )
 
